# -*- coding: utf-8 -*-
"""Data Task 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FijDbrxkFAU6m60LPL2Gok2u666RhLwx

# CMPE428 Assignment 3
Building Logistic Regression Classifiers

Imports
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm 
import step

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score

"""Import CSV"""

df = pd.read_csv('stdData.csv')

df.head()

"""## Task 1

### Split Dataset with Equal Positives and Negatives

We can see the amount of negatives and positives with this command
"""

df.Label.value_counts()

"""#### Replacing Categorical with Binary"""

df["Label"] = df["Label"].replace({"positive":1,"negative":0})

df.head()

"""#### Splitting Dataset Into 2, with Equal Amounts of Negative and Positive

We use stratify on y and split our dataframe into 2, so that we get equal distribution of negatives and positives.
"""

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.5, stratify = y)

y_train.value_counts()

y_test.value_counts()

"""We can see that numbers of 0 and 1 is equal across test and train.

### Logistic Regression

We will use sklearn's logistic in order to get our accuracy, f1, recall and precision scores.
"""

regression = LogisticRegression(solver = "liblinear")

model = regression.fit(X_train, y_train)
y_pred = model.predict(X_test)

"""### Computing Scores

Scores are calculated and inserted into the dataframe to be easily compared with the other model's scores later on.
"""

scores = {}
scores['Accuracy'] = accuracy_score(y_test, y_pred)
scores['F1'] = f1_score(y_test, y_pred)
scores['Recall'] = recall_score(y_test, y_pred)
scores['Precision'] = precision_score(y_test, y_pred)

scores = pd.DataFrame.from_dict(scores, orient='index', columns=['First DF'])
scores = scores.transpose()
scores

"""## Task 2

#### Identify Weak Variables by P-Value

Since sklearn doesn't have a way to check P-Value, I will use statsmodels to check P Values.

It can be seen that highest P values are V4,V5,V6 and V7 therefore it is the weakest data.
"""

model = sm.Logit(y_train, X_train).fit()
model.summary()

"""#### Using Backward Elimination

"""

step.forwardSelection(X_train, y_train)

"""After the backward elimination, we came to conclusion that our final variables will be V2, V5 and V1."""

new_x = X[['V1','V2','V5']]
new_x.head()

"""#### Generating New Model

We normally split data by 0.8 for train and 0.2 for test, but I will assume that it is required that we still have equal amounts of negatives and positives like in the Task 1.
"""

X_train, X_test, y_train, y_test = train_test_split(new_x, y, test_size = 0.5, stratify = y)

new_model = regression.fit(X_train, y_train)
y_pred_new = model.predict(X_test)

"""#### New Scores

Here we calculate scores and append the new scores into our dataframe. 
0th row is first and 1st row is second logistic model results.
"""

new_scores = {}
new_scores['Accuracy'] = accuracy_score(y_pred_new, y_pred)
new_scores['F1'] = f1_score(y_pred_new, y_pred)
new_scores['Recall'] = recall_score(y_pred_new, y_pred)
new_scores['Precision'] = precision_score(y_pred_new, y_pred)

all_scores = scores.append(new_scores, ignore_index=True)
all_scores

"""In conclusion, we can see our accuracy, F1, Recall and Precision has increasd slightly after doing a backward step elimination. This shows that our new model has increased performance and accuracy now."""

